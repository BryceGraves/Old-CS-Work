# -*- coding: utf-8 -*-
"""Lecture 03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10aJR-P7E4VlYbMEpMX10FL7nx6XKgqfM

# Lecture 03 - Datasets

***
##### CS 434 - Dating Mining and Machine Learning
##### Oregon State University-Cascades
***

## <img src="https://img.icons8.com/color/32/000000/fork-lift.png"/> Load packages
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

"""# <img src="https://img.icons8.com/color/32/000000/abc.png"/> Handling categorical data"""

# load categorial data
df = pd.DataFrame([['green', 'M', 10.1, 'class2'],
                   ['red', 'L', 13.5, 'class1'],
                   ['blue', 'XL', 15.3, 'class2']])

df.columns = ['color', 'size', 'price', 'classlabel']
df

"""## <img src="https://img.icons8.com/color/32/000000/numerical-sorting-12.png"/> Mapping ordinal features
Machine learning systems can not readily understand ordinal features when represented as categorial strings. We need to remap these strings to numbers so the system can interpret the ordinal series.
"""

# define a mapping
size_mapping = {'XL': 3,
                'L': 2,
                'M': 1}

# apply the mapping
df['size'] = df['size'].map(size_mapping)
df

# mapping back to original strings
inv_size_mapping = {v: k for k, v in size_mapping.items()}
df['size'].map(inv_size_mapping)

"""## <img src="https://img.icons8.com/color/32/000000/change.png"/> Encoding class labels
Some machine learning algorithms require class labels encoded as integer values.
"""

# create a mapping dict to convert class labels from strings to integers
class_mapping = {label: idx for idx,
                 label in enumerate(np.unique(df['classlabel']))}
class_mapping

"""### <img src="https://img.icons8.com/color/32/000000/panda.png"/> Label mapping with Pandas"""

# convert class labels from strings to integers using the mapping
df['classlabel'] = df['classlabel'].map(class_mapping)
df

# reverse the class label mapping
inv_class_mapping = {v: k for k, v in class_mapping.items()}
df['classlabel'] = df['classlabel'].map(inv_class_mapping)
df

"""### <img src="https://img.icons8.com/color/24/000000/test-tube.png"/> Label mapping with sklearn"""

# Label encoding with sklearn's LabelEncoder
class_le = LabelEncoder()
y = class_le.fit_transform(df['classlabel'].values)
y

# reverse mapping
class_le.inverse_transform(y)

"""# <img src="https://img.icons8.com/color/32/000000/fire-element.png"/> Performing one-hot encoding on nominal features
The `LabelEncoder` can be used to covert nominal features to integers.
"""

X = df[['color', 'size', 'price']].values
color_le = LabelEncoder()
X[:, 0] = color_le.fit_transform(X[:, 0])
X

X = df[['color', 'size', 'price']].values
color_ohe = OneHotEncoder()
color_ohe.fit_transform(X[:, 0].reshape(-1, 1)).toarray()

X = df[['color', 'size', 'price']].values
c_transf = ColumnTransformer([('onehot', OneHotEncoder(), [0]),
                              ('nothing', 'passthrough', [1, 2])])
c_transf.fit_transform(X).astype(float)

# multicollinearity guard for the OneHotEncoder
color_ohe = OneHotEncoder(categories='auto', drop='first')
c_transf = ColumnTransformer([('onehot', color_ohe, [0]),
                              ('nothing', 'passthrough', [1, 2])])
c_transf.fit_transform(X).astype(float)

# one-hot encoding via pandas
pd.get_dummies(df[['price', 'color', 'size']])

# multicollinearity guard in get_dummies
pd.get_dummies(df[['price', 'color', 'size']], drop_first=True)

"""***
## <img src="https://img.icons8.com/color/32/000000/chat.png"/> Discussion

1. What is the difference between numeric and categorial data?

2. What is the difference between nominal and ordinal data?

3. Why would you perform one-hot encoding?
***

# <img src="https://img.icons8.com/color/32/000000/split-files.png"/> Partitioning into train and test sets

## <img src="https://img.icons8.com/color/32/000000/database-restore.png"/>  Load some data
"""

df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

print('Class labels', np.unique(df_wine['Class label']))
df_wine.head()

"""## <img src="https://img.icons8.com/color/32/000000/fit-to-width.png"/> Overfitting and Underfitting

* Underfitting occurs when the model fails to capture the underlying patterns in the data.
* Overfitting occurs when a model captures the patterns in the dataset well, but fails to generalize well to unseen data.

![alt text](https://miro.medium.com/max/555/1*tBErXYVvTw2jSUYK7thU2A.png)

## <img src="https://img.icons8.com/color/32/000000/split.png"/> Split features and labels

To assess a model's ability to generalize to new data, we partition the dataset into
* **train** set: data used to train the model
* **test** set: data held out to evaluate the model
"""

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

print('X shape', X.shape)
print('y shape', y.shape)

"""## <img src="https://img.icons8.com/color/32/000000/split-files.png"/> Split train and test sets

<img src=https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch04/images/04_02.png width="500">
"""

X_train, X_test, y_train, y_test =\
    train_test_split(X, y,
                     test_size=0.3,
                     random_state=0,
                     stratify=y)

print('X_train shape', X_train.shape)
print('y_train shape', y_train.shape)
print()
print('X_test shape', X_test.shape)
print('y_test shape', y_test.shape)

"""***
## <img src="https://img.icons8.com/color/32/000000/chat.png"/> Discussion

1. Why do we split into training and test sets?
1. Why is `X` captialized but `y` is not?
1. Where do we stratify the data? Why?

***

# <img src="https://img.icons8.com/color/32/000000/scales.png"/> Bringing features onto the same scale

Most machine learning models require feature values to be on the same scale.

<img src=https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch04/images/04_01.png width="500">

> Decision trees and random forests are one of the few algorithms that don't require feature scaling.

## <img src="https://img.icons8.com/color/32/000000/width.png"/>  Normalize

Normalization is min-max scaling to range $[0,1]$.

$$ x_{norm}^i = {{x^i - x_{min}} \over {x_{max} - x_{min}}}$$

where
* $x^i$ is a particular example
* $x_{min}$ is the smallest value
* $x_{max}$ is the largest value
"""

mms = MinMaxScaler()
X_train_norm = mms.fit_transform(X_train)
X_test_norm = mms.transform(X_test)

print(X_train_norm)

"""## <img src="https://img.icons8.com/color/32/000000/compare.png"/> Standardize

Standardization recenters the feature column at mean $0$ with a standard deviation of $1$.

$$x^i_{std} = {{x^i - \mu_x} \over \sigma_x} $$

where:
* $\mu_x$ is the sample mean of the column
* $\sigma_x$ is the corresponding standard deviation
"""

stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.transform(X_test)

"""> Standardization preserves information at outliers that normalization does not.

## <img src="https://img.icons8.com/color/32/000000/thinking-bubble.png"/> Thought exercise

#### <img src="https://img.icons8.com/color/32/000000/question-shield.png"/> Question
What if we accidentally wrote `X_test_std = sc.fit_transform(X_test)` instead of `X_test_std = sc.transform(X_test)`? 

In this case, it wouldn't make a big difference since the mean and standard deviation of the test set should be (quite) similar to the training set. However, the correct way is to re-use parameters from the training set if we are doing any kind of transformation -- the test set should be "new, unseen" data.

#### <img src="https://img.icons8.com/color/32/000000/error.png"/>  Warning
This typo reflects a common mistake that some people are not re-using these parameters from the model training/building and standardize the new data "from scratch." Here's simple example to explain why this is a problem.

#### <img src="https://img.icons8.com/color/32/000000/new-document.png"/> Example

Let's assume we have a simple training set consisting of 3 examples with 1 feature (let's call this feature "length"):

* train_1: 10 cm -> class_2
* train_2: 20 cm -> class_2
* train_3: 30 cm -> class_1
mean: 20, std.: 8.2

After standardization, the transformed feature values are

* train_std_1: -1.21 -> class_2
* train_std_2: 0 -> class_2
* train_std_3: 1.21 -> class_1

Next, let's assume our model has learned to classify examples with a standardized length value < 0.6 as class_2 (class_1 otherwise). So far so good. Now, let's say we have 3 unlabeled data points that we want to classify:

* new_4: 5 cm -> class ?
* new_5: 6 cm -> class ?
* new_6: 7 cm -> class ?

If we look at the "unstandardized "length" values in our training datast, it is intuitive to say that all of these examples are likely belonging to class_2. However, if we standardize these by re-computing standard deviation and and mean you would get similar values as before in the training set and your classifier would (probably incorrectly) classify examples 4 and 5 as class 2.

* new_std_4: -1.21 -> class 2
* new_std_5: 0 -> class 2
* new_std_6: 1.21 -> class 1

However, if we use the parameters from your "training set standardization," we'd get the values:

* example5: -18.37 -> class 2
* example6: -17.15 -> class 2
* example7: -15.92 -> class 2

The values 5 cm, 6 cm, and 7 cm are much lower than anything we have seen in the training set previously. Thus, it only makes sense that the standardized features of the "new examples" are much lower than every standardized feature in the training set.

##<img src="https://img.icons8.com/color/32/000000/code.png"/> Example

An example using the direct formulae with `numpy`.
"""

ex = np.array([0, 1, 2, 3, 4, 5])

# original
print('data:', ex, '', sep='\n')

# standardize
print('standardized:', (ex - ex.mean()) / ex.std(), '', sep='\n')

# normalize
print('normalized:', (ex - ex.min()) / (ex.max() - ex.min()), sep='\n')

"""***
## <img src="https://img.icons8.com/color/32/000000/chat.png"/> Discussion
1. Why do we standardize or normalize?
1. When should you normalize vs standardize?
1. Why don't we base the transformation on all the data?
***
"""
