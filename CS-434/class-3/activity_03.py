# -*- coding: utf-8 -*-
"""Activity 03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/169MCNIOHR99-wbgB7VWJutzSga9xLzvJ

#Activity 03 - Datasets

***
##### CS 434 - Dating Mining and Machine Learning
##### Oregon State University-Cascades
***

# Load Packages
"""

import ssl
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
ssl._create_default_https_context = ssl._create_unverified_context

"""# Dataset

### Location

https://archive.ics.uci.edu/ml/datasets/census+income

### Description

Predict whether income exceeds $50K/yr based on census data. Also known as "Adult" dataset.

> Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: `((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))`

### Attributes
* `age`: continuous
* `workclass`: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked
* `fnlwgt`: continuous
* `education`: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.
* `education-num`: continuous.
* `marital-status`: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
* `occupation`: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.
* `relationship`: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.
* `race`: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.
* `sex`: Female, Male.
* `capital-gain`: continuous.
* `capital-loss`: continuous.
* `hours-per-week`: continuous.
* `native-country`: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.

### Class label

* `>50K, <=50K`: binary label, true if annual income is below $50k
"""

attributes = ['age', 'workclass', 'fnlwgt', 'education', 'education-value', 'marital-status', 'occupation',
              'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']
url = '../data/adult.data'

"""*** 
# Exercise #1 - Load data
***

##### 1.1 Load the dataset from the url and `display(df)`.
"""

# load the dataset into a dataframe
df = pd.read_csv(url, names=attributes)
print(df)

clean_df = df

"""##### 1.2 Count the number of records"""

# count number of records
print(df.shape[0])

"""##### 1.3 Create a mapping from the `'income'` strings to integers"""

# create a mapping dict to convert class labels from strings to integers
income_map = {' <=50K': 1, ' >50K': 0}

"""##### 1.4 Convert `'income'` strings to integers"""

# convert class labels from strings to integers using the mapping
clean_df['income'] = df['income'].map(income_map)

"""##### 1.5 List the `unique()` values of `'education'`"""

# list the unique values of 'education'
print(df['education'].unique())

"""> Note the extra space in the string.  We'll convert this data column in the next section.

*** 
# Exercise #2 - Mapping ordinal data
***

##### 2.1 Count the values in `'education`' using `value_counts()`
"""

# count the values in 'education'
print(clean_df['education'].value_counts())

"""##### 2.2 Select and display only the `'education'` and `'education-value'` columns"""

# display two columns: 'education' and 'education-value'
print(clean_df[['education', 'education-value']])

"""> Notice that `'eduction-value'` is an ordinal representation of `'education'`.
>
> For practice, let's convert the `'education'` column to an ordinal numbering and then we can check it against `'education-value'`.

##### 2.3 Define a map that maps the categorical `'education'` value to an ordinal number. For example `' Preschool' : 1'`.
"""

# define an ordinal mapping for education
education_mapping = {
    ' Preschool': 1,
    ' 1st-4th': 2,
    ' 5th-6th': 3,
    ' 7th-8th': 4,
    ' 9th': 5,
    ' 10th': 6,
    ' 11th': 7,
    ' 12th': 8,
    ' HS-grad': 9,
    ' Some-college': 10,
    ' Assoc-voc': 11,
    ' Assoc-acdm': 12,
    ' Bachelors': 13,
    ' Masters': 14,
    ' Prof-school': 15,
    ' Doctorate': 16
}

"""> Again note the extra space in the string.

##### 2.4 Apply the ordinal mapping using `map`
"""

# apply the mapping and display dataframe
clean_df['education'] = df['education'].map(education_mapping)

print(clean_df)

"""##### 2.5 Confirm your new mapping of `'education'` matches the given mapping."""

# True, if the two columns match
print(clean_df['education'].equals(clean_df['education-value']))

"""***
# Exercise #3 - One-hot encoding 
***

##### 3.1 Use to `get_dummies(.)` for `'sex'` and store in new dataframe `dummies`
"""

# create dummies for 'sex'
dummies = pd.get_dummies(df['sex'])

"""##### 3.2 Drop column 'sex'"""

# drop column 'sex' as it is now encoded
clean_df.drop(columns='sex', inplace=True)

"""##### 3.3 `join(.)` dataframe `dummies` with `df` and `display`"""

# join the encoded df dummies with df
clean_df = clean_df.join(dummies)

"""##### 3.4 Rename columns to be named `{male, female}`

> **Example**: rename columns
> <pre>$df = df.rename({'a': 'x', 'b': 'y'}, axis=1)</pre>
"""

# rename to the lower case equivalent
clean_df = clean_df.rename(str.lower, axis='columns')
print(clean_df)

"""##### 3.5 Find the sums of the `female` and the `male` columns, respectively"""

# count female
print('Female count: ', clean_df[' female'].sum())

# count male
print('Male count: ', clean_df[' male'].sum())


"""***
# Exercise #4 - Features on same scale
***

##### 4.1 Describe the summary statistics for `'hours-per-week'`
"""

# describe 'hours-per-week'
print(clean_df['hours-per-week'].describe())

"""###### 4.2 Standardize the column `'hour-per-week'`"""

# use StandardScaler to fit tranform the column 'hours-per-week'
standard_scaler = StandardScaler()
clean_df['hours-per-week'] = standard_scaler.fit_transform(
    clean_df['hours-per-week'].values.reshape(-1, 1))

"""##### 4.3 Describe the summary statistics for `'hours-per-week'` again"""

# describe 'hours-per-week' again
print(clean_df['hours-per-week'].describe())

"""##### 4.4 Standardize `'captial-gain'` and `'capital-loss'`"""

# standardize 'captial-gain' and 'capital-loss'
clean_df[['capital-gain', 'capital-loss']] = standard_scaler.fit_transform(
    clean_df[['capital-gain', 'capital-loss']].values)

"""##### 4.5 Describe the summary statistics for `'captial-gain'` and `'capital-loss'`"""

# describe 'captial-gain' and 'capital-loss'
print(clean_df[['capital-gain', 'capital-loss']].describe())

"""***
# Exercise #5 - Partition into train and test sets
***

##### 5.1 Partition in into `X` (features) and `y` (class labels)
"""

# partition into attributes X and class labels y
X, y = df.loc[:, df.columns != 'income'].values, df['income']

print('X: ', X)
print('y: ', y)

"""##### 5.2 Split into train and test (0.3), stratified by `y`"""

# split into train and test (both X and y)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0, stratify=y
)

"""##### 5.3 Count the number of examples in `X_train`"""

# count number of examples in X_train
print(X_train.shape[0])

"""##### 5.4 Count the number of examples in `X_test`"""

# count number of examples in X_test
print(X_test.shape[0])

"""##### 5.5 Verify the size of `X_test` is $ \approx{0.3} $ of the total number of examples"""

# divide |X_test| by (|X_train|+|X_test|)
print(X_test.shape[0] / (X_train.shape[0] + X_test.shape[0]))

"""<img src="https://66.media.tumblr.com/dded9d1a2bf2068f92af9f7a9b6b5451/tumblr_p6s3hbPzgV1vd8jsjo1_500.gifv" width="300">"""
