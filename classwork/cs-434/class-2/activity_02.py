# -*- coding: utf-8 -*-
"""Activity 02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Y2uFprh0y-mLMjVBUwZBtfz7F2xL3PI

#Activity 02 - Data Processing

***
##### CS 434 - Dating Mining and Machine Learning
##### Oregon State University-Cascades
***

# Instructions
Complete this "worksheet" by writing code to complete the series of tasks mentioned.  Then open the Canvas "quiz" Activity (of the same name) and answer the questions based on the results.

# Load packages
"""

import pandas as pd
import numpy as np

# personal imports
from IPython.display import display

"""*** 

# Dataset

***

### Location
../data/travel-times.csv

### Description
A driver uses an app to track GPS coordinates as he drives to work and back each day. The app collects the location and elevation data. Data for about 200 trips are summarized in this data set.

### Attributes
* `Date` : date of travel
* `StartTime` : when getting into the car
* `DayOfWeek`: the day name
* `GoingTo`: direction of travel
* `Distance`: distance travelled in kilometers
* `MaxSpeed`: fastest speed recorded (all trips are on the 407 highway for some portion)
* `AvgSpeed`: the average speed for the entire trip
* `AvgMovingSpeed`: the average speed recorded only while the car is moving
* `FuelEconomy`: a rough estimate of fuel economy (it is inaccurate)
* `TotalTime`: duration of the entire trip, in minutes
* `MovingTime`: duration when the car was considered to be moving (i.e. not counting traffic delays, accidents, or time while the car is stationary)
* `Take407All`: is Yes if the 407 toll highway was taken for the entire trip.
> *I try to avoid taking the 407, taking slower back routes to save costs. But some days I'm running late, or just lazy, and take it all the way.*
* `Comments` : miscellaneous notes
"""

url = '../data/travel-times.csv'

"""*** 
# Exercise #1 - Examine data by row
***

##### 1.1 Load the dataset from the url and `display(df)`.
"""

# load and display dataset
df = pd.read_csv(url)
display(df)

"""##### 1.2. Print the number of rows."""

# count the number of rows
print('Number of rows: ', len(df))

"""##### 1.3. Count number of rows that contain a missing value"""

# count the number of rows that are missing value(s)
print('Number of rows that are missing values: ',
      df.shape[0] - df.dropna().shape[0])

"""##### 1.4. Drop the rows that contain a missing value"""

# remove rows that contain missing values
clean_df = df.dropna(axis=0)

print(clean_df)

"""##### 1.5. Count the number of rows (again)"""

# count the number of rows
print('Number of rows after drop: ', len(clean_df))

"""> Conclusion. That's not very much data.  Let's try again.

*** 
# Exercise #2 - Examine data by column
***

##### 2.1. Reload the dataset from the url and `display(df)`.
"""

# load and display dataset
print('The original df is still in place: ', df)

"""##### 2.2. Print the number of columns"""

# count the number of columns
print('Number of columns: ', df.shape[1])

"""##### 2.3. Count the number of missing values by column"""

# count the number of missing values by column
print('Number of missing values by column:\n', df.isnull().sum())

"""##### 2.4. Drop the columns that contain a missing value"""

# remove columns that contain missing values
cleaned_df_by_column = df.dropna(axis=1)

"""##### 2.5. Recount the number of columns"""

# count the number of columns
print('Number of columns after removal: ', cleaned_df_by_column.shape[1])

"""> Conclusion.  That's better, but we're still throwing away data. Let's try again.

*** 
# Exercise #3 - Remove a data column
***

##### 3.1. Reload the dataset from the url and `display(df)`
"""

# load and display dataset
print('The original df is still in place: ', df)

"""##### 3.2. Count the total number of missing values"""

# count the total number of missing values
print('Total number of missing values: ', df.isnull().sum().sum())

"""##### 3.3. Drop the `'Comments'` column"""

# remove the "Comments" column
df_without_comments = df.drop(columns='Comments')
print(df_without_comments)

"""##### 3.4. Count the total number of missing values (again)"""

# count the total number of missing values
print('New count of missing values', df_without_comments.isnull().sum().sum())

"""##### 3.5. Examine the shape of the dataframe"""

# print the shape
print('df shape: ', df_without_comments.shape)

"""*** 
# Exercise 4 - Handling missing data
***

> **Tip**: use <img src="https://img.icons8.com/color/20/000000/google-logo.png"/> to learn how to do things in <img src="https://img.icons8.com/color/20/000000/panda.png"/>.

##### 4.0. Continue with the dataframe (without the `'Comments'` column)

"""

df = df_without_comments

"""

##### 4.1. Impute the missing values with attribute mean and `display(df)`
"""

# fill NaNs with the mean
print("You can't calculate the mean on a column with non numeric values.")

"""> Observe `'FuelEconomy'`. There are still `NaN`s. Why didn't that work?

##### 4.2. Examine the type of `'FuelEconomy'` column.
"""

# print the dtype of 'FuelEconomy' column
print('FuelEconomy dtype: ', df['FuelEconomy'].dtype)

"""> Insight: that means we have a mixture of strings and numbers

##### 4.3. Print the `to_numpy()` representation of the `'FuelEconomy'` column.
"""

# print the numpy representation
print('Numy representation: ', df['FuelEconomy'].to_numpy())

"""> Do you spot the problem?

##### 4.4.  Use `regex=True` to replace `-` with `np.nan`
"""

# replace the dash with a NaN
df['FuelEconomy'] = df['FuelEconomy'].replace(
    to_replace=r'^-$', value=np.nan, regex=True)

"""##### 4.5. Convert the `FuelEconomy` using `pd.to_numeric(.)`"""

# convert 'FuelEconomy' to numeric

df['FuelEconomy'] = pd.to_numeric(df['FuelEconomy'])

"""*** 

# Exercise #5 - Impute data

***

##### 5.1. Impute the missing values with attribute mean and `display(df)` (again)
"""

# fill NaNs with the mean
clean_df = df.fillna(df.mean())
display(clean_df)

"""##### 5.2. Print the average `'FuelEconomy'`"""

# print the average 'FuelEconomy'
print('FuelEconomy mean: ', clean_df['FuelEconomy'].mean())

"""##### 5.3. Print the median `'FuelEconomy'`"""

# print the median 'FuelEconomy'
print('FuelEconomy median: ', clean_df['FuelEconomy'].median())

"""##### 5.4. Print the standard deviation of `'FuelEconomy'`"""

# print the st.dev of 'FuelEconomy'
print('FuelEconomy standard deviation: ', clean_df['FuelEconomy'].std())

"""##### 5.5. describe the summary statistics of `'FuelEconomy'`"""

# print summary statistics
print('FuelEconomy summary statistics: ', clean_df['FuelEconomy'].describe())

"""<img src="https://66.media.tumblr.com/dded9d1a2bf2068f92af9f7a9b6b5451/tumblr_p6s3hbPzgV1vd8jsjo1_500.gifv" width="300">"""
