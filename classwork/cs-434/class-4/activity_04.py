# -*- coding: utf-8 -*-
"""Activity 04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OxyDyhj0R88t5xYVc0NDH9-j44a947_l

# Activity 04 - Features Engineering

***
##### CS 434 - Dating Mining and Machine Learning
##### Oregon State University-Cascades
***

# Load packages
"""

import ssl
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
ssl._create_default_https_context = ssl._create_unverified_context

"""# Dataset

### Overview
Dataset from the USA Forensic Science Service; 6 types of glass; defined in terms of their oxide content (i.e. Na, Fe, K, etc)

The study of classification of types of glass was motivated by criminological investigation. At the scene of the crime, the glass left can be used as evidence...if it is correctly identified!

### Attributes

* `id`: number: 1 to 214
* `RI`: refractive index
* `Na`: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)
* `Mg`: Magnesium
* `Al`: Aluminum
* `Si`: Silicon
* `K`: Potassium
* `Ca`: Calcium
* `Ba`: Barium
* `Fe`: Iron

### Class label

* `Type of glass`: class attribute
1. building_windows_float_processed
2. building_windows_non_float_processed
3. vehicle_windows_float_processed
4. vehicle_windows_non_float_processed (none in this database)
5. containers
6. tableware
7. headlamps
"""

url = '../data/glass.data'
attributes = [
    'id',
    'RI',
    'Na',
    'Mg',
    'Al',
    'Si',
    'K',
    'Ca',
    'Ba',
    'Fe',
    'Type'
]

"""*** 
# Exercise #1 - Load data
***

##### 1.1 Load the dataset from the url and `display(df)`.
"""

# load the dataset into a dataframe
df = pd.read_csv(url, names=attributes)
clean_df = df

"""##### 1.2 Drop the 'id' column"""

# drop 'id'
clean_df.drop(columns='id', inplace=True)

"""> The `id` isn't a descriptive feature, so we need to drop it from the dataset.

##### 1.3 Describe the dataset
"""

# describe the dataset
print(clean_df.describe())

"""*** 
# Exercise #2 - Log transform
***

##### 2.1 Create a `pairplot` for all `float64` features using `seaborn`
"""

# make a pairplot of all features (and wait for it)
sns.pairplot(clean_df)
plt.title('Pairplot')
plt.show()

"""> Note the histograms along the diagonal.  Many of the distributions are skewed.

##### 2.2 Create and show a `distplot` for `'MG'`
"""

sns.distplot(clean_df['RI'])
plt.title('RI')
plt.show()

# show a distplot of 'Mg' with title
sns.distplot(clean_df['Mg'])
plt.title('Mg')
plt.show()

"""> Every graph needs a title. Use `plt.title('Mg')`.

##### 2.3 Log transform `'Mg'` and update the column in the dataframe
"""

# use numpy log transform on 'Mg'
clean_df['Mg'] = np.log1p(clean_df['Mg'])

"""> **Protip:** Practice the log transform by first saving to a new variable.  Only when you are sure you are correct should you update the original dataframe with the log-transform. If you make a mistake, you will need to `Run all` to reload the original dataset.

##### 2.4 Create and show a `distplot` for `'Mg'` (again)
"""

# show a distplot of 'Mg' with title
sns.distplot(clean_df['Mg'])
plt.title('Mg')
plt.show()

"""> There was no major benefit to log-transform in this case, except to practice.

*** 
# Exercise #3 - Discretize
***

##### 3.1 Split into features `X` and labels `y`

> Make sure your class label ends up in `y` and the rest in `X`
"""

# split into X and y
X, y = clean_df.iloc[:, :-1].values, clean_df.iloc[:, -1].values

"""> **Protip**: when using `iloc` indexing, the value `-1` means the *last* and `:-1` means *all but the last*.

##### 3.2 Split into train and test sets with `test_size=0.1`, stratified by `y`
"""

# split into train and test
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.1,
    random_state=0,
    stratify=y
)

"""##### 3.3 Create 10 equal width bins and print the count for each bin"""

# create 10 equal width bins
discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')
discretizer.fit(X_train)
X_train_equal_width = discretizer.transform(X_train)
X_test_equal_width = discretizer.transform(X_test)

for i in range(0, 10):
    print('bin', i, ':', np.count_nonzero(X_train_equal_width == i))

"""##### 3.4 Create 5 equal frequency bins and print the count for each bin"""

# create 5 equal frequency bins
discretizer = KBinsDiscretizer(
    n_bins=5, encode='ordinal', strategy='quantile')
discretizer.fit(X_train)
X_train_equal_frequency_quantile = discretizer.transform(X_train)
X_test_equal_frequency_quantile = discretizer.transform(X_test)

for i in range(0, 10):
    print('bin', i, ':', np.count_nonzero(
        X_train_equal_frequency_quantile == i))

"""> Note: the above should generate some warnings about small bins. It happens because our toy dataset is very small.

##### 3.5 Create 5 equal frequency bins with `kmeans` and print the count for each bin
"""

# create 5 equal frequency bins with kmeans
discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='kmeans')
discretizer.fit(X_train)
X_train_equal_frequency_kmeans = discretizer.transform(X_train)
X_test_equal_frequency_kmeans = discretizer.transform(X_test)

for i in range(0, 5):
    print('bin', i, ':', np.count_nonzero(X_train_equal_frequency_kmeans == i))

"""*** 
# Exercise #4 -Feature importance
***

##### 4.1 Extract the feature names from `df.columns`
"""

# extract the feature names from df as 'feat_labels'
feat_labels = clean_df.columns[1:]

"""##### 4.2 Fit the data to a random forest with `n_estimators=500`"""

# fit the data to a RandomForestClassifier
forest = RandomForestClassifier(n_estimators=500, random_state=1)
forest.fit(X_train, y_train)

"""##### 4.3 Extract the importances and sort the indices"""

# extract the importances
importances = forest.feature_importances_

# sort the indices by importance (hi to lo)
indices = np.argsort(importances)[::-1]

"""##### 4.4 Print the feature label and its importance"""

# print the feature labels and importance
for f in range(X_train.shape[1]):
    print("%2d) %-*s %f" %
          (f + 1, 30, feat_labels[indices[f]], importances[indices[f]]))

"""##### 4.5 Make a bar chart of the feature importance"""

# show a bar chart of feature importance (hi to lo)
plt.title('Feature Importance')
plt.bar(range(X_train.shape[1]),
        importances[indices],
        align='center')

plt.xticks(range(X_train.shape[1]),
           feat_labels[indices], rotation=90)
plt.xlim([-1, X_train.shape[1]])
plt.tight_layout()
plt.show()

"""<img src="https://66.media.tumblr.com/dded9d1a2bf2068f92af9f7a9b6b5451/tumblr_p6s3hbPzgV1vd8jsjo1_500.gifv" width="300">"""
