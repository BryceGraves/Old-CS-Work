# -*- coding: utf-8 -*-
"""Lecture 04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WUizYYy4-4BK-EFsjfRByPgWL1b1s8p4

#Lecture 04 - Feature Engineering

***
##### CS 434 - Dating Mining and Machine Learning
##### Oregon State University-Cascades
***

## <img src="https://img.icons8.com/color/32/000000/fork-lift.png"/> Load packages
"""

import ssl
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.base import clone
from itertools import combinations
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import KBinsDiscretizer
ssl._create_default_https_context = ssl._create_unverified_context

"""# <img src="https://img.icons8.com/color/32/000000/wine-glass.png"/> Wine dataset

**Description**: chemical analysis to determine the origin of wines.

**Source**: https://archive.ics.uci.edu/ml/datasets/Wine

### <img src="https://img.icons8.com/color/32/000000/accept-database.png"/> Load dataset
"""

df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

print('Class labels', np.unique(df_wine['Class label']))
display(df_wine.head())

"""### <img src="https://img.icons8.com/color/32/000000/split.png"/> Split into train and test sets"""

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

X_train, X_test, y_train, y_test =\
    train_test_split(X, y,
                     test_size=0.3,
                     random_state=0,
                     stratify=y)

print(len(X_train), len(X_test))

"""### <img src="https://img.icons8.com/color/32/000000/scales.png"/> Standardize"""

stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.transform(X_test)

"""***
# <img src="https://img.icons8.com/color/32/000000/change.png"/> Feature transformation
***

### <img src="https://img.icons8.com/color/32/000000/data-sheet.png"/> Print dataframe
"""

df_wine

"""Sometimes it is necessary to transform the feature values for use in models.

## <img src="https://img.icons8.com/color/32/000000/metamorphose.png"/> Log-transform features
"""

sns.distplot(df_wine['Proanthocyanins'])
plt.title('Histogram of Proanthocyanins')
plt.show()

df_wine['Proanthocyanins'] = np.log1p(df_wine['Proanthocyanins'])

sns.distplot(df_wine['Proanthocyanins'])
plt.title("Log-Transformed plot of Proanthocyanins")
plt.show()

"""## <img src="https://img.icons8.com/color/32/000000/split-table.png"/> Feature discretization

Feature discretization decomposes each feature into a set of bins, allowed continuous data to be treated as a finite set of discrete values.

### <img src="https://img.icons8.com/color/32/000000/width.png"/> Equal width binning

<font color="#D73F09"><b>Equal width binning</b></font> picks a constant bin width across the range of the values.


$$ \text{width} = {{\text{maximum value} - \text{minimum value}} \over {N}} $$
"""

# create 10 equal width bins
discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')
discretizer.fit(X_train_std)
X_train_eqwd = discretizer.transform(X_train_std)
X_test_eqwd = discretizer.transform(X_test_std)

# how many per bin
for i in range(0, 10):
    print('bin', i, ':', np.count_nonzero(X_train_eqwd == i))

"""### <img src="https://img.icons8.com/color/32/000000/abacus.png"/> Equal frequency discretization

<font color="#D73F09"><b>Equal frequency binning</b></font> picks bin boundaries such that there are approximately equal number of values in each bin.
"""

discretizer = KBinsDiscretizer(
    n_bins=10, encode='ordinal', strategy='quantile')
discretizer.fit(X_train_std)
X_train_eqfr = discretizer.transform(X_train_std)
X_test_eqfr = discretizer.transform(X_test_std)

# how many per bin
for i in range(0, 10):
    print('bin', i, ':', np.count_nonzero(X_train_eqfr == i))

"""### <img src="https://img.icons8.com/color/32/000000/k.png"/> K-means discretization

<font color="#D73F09"><b>K-means binning</b></font> using unsupervised clustering to define bins.
"""

discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='kmeans')
discretizer.fit(X_train_std)
X_train_eqfr = discretizer.transform(X_train_std)
X_test_eqfr = discretizer.transform(X_test_std)

# how many per bin
for i in range(0, 5):
    print('bin', i, ':', np.count_nonzero(X_train_eqfr == i))

"""***
# <img src="https://img.icons8.com/color/32/000000/sorting-answers.png"/> Feature selection
***

## <img src="https://img.icons8.com/color/32/000000/edit-row.png"/> Approach 1: Regularization

Regularization tunes the complexity of a model by adding a penalty term to the cost function of the model to encourage smaller weights by penalizing large weights.

Regularization is a useful method for 
* handling collinearity (high correlation among features)
* filtering out noise from data
* prevent overfitting

### Sum of square errors
The goal is to find the combination of weight coefficients that minimize the cost function for training data.

<img src=https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch04/images/04_04.png width="500">

### $L_2$ regularization

$$\lVert \mathbf{w} \rVert^2_2 = \sum_{j=1}^m \  w_j^2 $$

<img src=https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch04/images/04_05.png width="500">

### $L_1$ regularization

$$\lVert \mathbf{w} \rVert_1 = \sum_{j=1}^m \ \lvert w_j \rvert $$

<img src=https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch04/images/04_06.png width="500">

> $L_1$ regularization usually yield sparse feature vectors and most feature weights will be zero.  Sparsity can be useful for high-dimensional datasets with many irrelevant features.

### <img src="https://img.icons8.com/color/32/000000/new-document.png"/>  Example

For regularized models in scikit-learn that support L1 regularization (e.g. `LogisticRegression`, we can simply set the `penalty` parameter to `'l1'` to obtain a sparse solution:
"""

LogisticRegression(penalty='l1', solver='liblinear', multi_class='ovr')

"""Apply to the standardized Wine data."""

lr = LogisticRegression(penalty='l1', C=1.0,
                        solver='liblinear', multi_class='ovr')
# Note that C=1.0 is the default. You can increase
# or decrease it to make the regulariztion effect
# stronger or weaker, respectively.
lr.fit(X_train_std, y_train)
print('Training accuracy:', lr.score(X_train_std, y_train))
print('Test accuracy:', lr.score(X_test_std, y_test))

"""View the intercepts (Class$_1$ vs Class$_{2,3}$, Class$_2$ vs Class$_{1,3}$, Class$_3$ vs Class$_{1,2}$)"""

lr.intercept_

"""View the weight array, one row for each class."""

lr.coef_

"""Graph the regularization strength."""

fig = plt.figure()
ax = plt.subplot(111)

colors = ['blue', 'green', 'red', 'cyan',
          'magenta', 'yellow', 'black',
          'pink', 'lightgreen', 'lightblue',
          'gray', 'indigo', 'orange']

weights, params = [], []
for c in np.arange(-4., 6.):
    lr = LogisticRegression(penalty='l1', C=10.**c, solver='liblinear',
                            multi_class='ovr', random_state=0)
    lr.fit(X_train_std, y_train)
    weights.append(lr.coef_[1])
    params.append(10**c)

weights = np.array(weights)

for column, color in zip(range(weights.shape[1]), colors):
    plt.plot(params, weights[:, column],
             label=df_wine.columns[column + 1],
             color=color)
plt.axhline(0, color='black', linestyle='--', linewidth=3)
plt.xlim([10**(-5), 10**5])
plt.ylabel('weight coefficient')
plt.xlabel('C')
plt.xscale('log')
plt.legend(loc='upper left')
ax.legend(loc='upper center',
          bbox_to_anchor=(1.38, 1.03),
          ncol=1, fancybox=True)
plt.show()

"""## <img src="https://img.icons8.com/color/32/000000/workflow.png"/>  Approach 2: Sequential feature selection

Another method to avoid overfitting is through dimensionality reduction via feature selection.
"""


class SBS():
    def __init__(self, estimator, k_features, scoring=accuracy_score,
                 test_size=0.25, random_state=1):
        self.scoring = scoring
        self.estimator = clone(estimator)
        self.k_features = k_features
        self.test_size = test_size
        self.random_state = random_state

    def fit(self, X, y):

        X_train, X_test, y_train, y_test = \
            train_test_split(X, y, test_size=self.test_size,
                             random_state=self.random_state)

        dim = X_train.shape[1]
        self.indices_ = tuple(range(dim))
        self.subsets_ = [self.indices_]
        score = self._calc_score(X_train, y_train,
                                 X_test, y_test, self.indices_)
        self.scores_ = [score]

        while dim > self.k_features:
            scores = []
            subsets = []

            for p in combinations(self.indices_, r=dim - 1):
                score = self._calc_score(X_train, y_train,
                                         X_test, y_test, p)
                scores.append(score)
                subsets.append(p)

            best = np.argmax(scores)
            self.indices_ = subsets[best]
            self.subsets_.append(self.indices_)
            dim -= 1

            self.scores_.append(scores[best])
        self.k_score_ = self.scores_[-1]

        return self

    def transform(self, X):
        return X[:, self.indices_]

    def _calc_score(self, X_train, y_train, X_test, y_test, indices):
        self.estimator.fit(X_train[:, indices], y_train)
        y_pred = self.estimator.predict(X_test[:, indices])
        score = self.scoring(y_test, y_pred)
        return score


"""### <img src="https://img.icons8.com/color/32/000000/module.png"/> Create and run a machine learning model"""

knn = KNeighborsClassifier(n_neighbors=5)

# selecting features
sbs = SBS(knn, k_features=1)
sbs.fit(X_train_std, y_train)

# plotting performance of feature subsets
k_feat = [len(k) for k in sbs.subsets_]

plt.plot(k_feat, sbs.scores_, marker='o')
plt.ylim([0.7, 1.02])
plt.ylabel('Accuracy')
plt.xlabel('Number of features')
plt.grid()
plt.tight_layout()
plt.show()

# how many susbsets
len(sbs.subsets_)

# consider best three features
k3 = list(sbs.subsets_[10])
print(df_wine.columns[1:][k3])

# all features
knn.fit(X_train_std, y_train)
print('Training accuracy:', knn.score(X_train_std, y_train))
print('Test accuracy:', knn.score(X_test_std, y_test))

# best three features
knn.fit(X_train_std[:, k3], y_train)
print('Training accuracy:', knn.score(X_train_std[:, k3], y_train))
print('Test accuracy:', knn.score(X_test_std[:, k3], y_test))

"""## <img src="https://img.icons8.com/color/32/000000/flow-chart.png"/> Approach 3: Finding feature importance with trees

This snippet runs a Random Forest Classifier to rank features by importance.
"""

feat_labels = df_wine.columns[1:]

forest = RandomForestClassifier(n_estimators=500,
                                random_state=1)

forest.fit(X_train, y_train)
importances = forest.feature_importances_

indices = np.argsort(importances)[::-1]

for f in range(X_train.shape[1]):
    print("%2d) %-*s %f" % (f + 1, 30,
                            feat_labels[indices[f]],
                            importances[indices[f]]))

plt.title('Feature Importance')
plt.bar(range(X_train.shape[1]),
        importances[indices],
        align='center')

plt.xticks(range(X_train.shape[1]),
           feat_labels[indices], rotation=90)
plt.xlim([-1, X_train.shape[1]])
plt.tight_layout()
plt.show()

"""### <img src="https://img.icons8.com/color/32/000000/electrical-threshold.png"/> Selecting with a threshold"""

# set a threshold to pick features
sfm = SelectFromModel(forest, threshold=0.1, prefit=True)
X_selected = sfm.transform(X_train)
print('Number of features that meet this threshold criterion:',
      X_selected.shape[1])

# display the selected features
for f in range(X_selected.shape[1]):
    print("%2d) %-*s %f" % (f + 1, 30,
                            feat_labels[indices[f]],
                            importances[indices[f]]))

"""***
# <img src="https://img.icons8.com/color/32/000000/chat.png"/> Discussion
1. When might you log-transform a feature?
1. When might you discretize data?
1. What are the pros and cons of discretization?
1. When would you consider equal-width  vs equal-frequency discretization?
1. Why perform feature selection instead of using all features?

***
"""
